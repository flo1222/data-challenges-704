{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 5 - Sentiment analysis with Word2Vec\n",
    "\n",
    "### Exercise objectives:\n",
    "- Convert words to vectors with Word2Vec\n",
    "- Discover Sentiment analysis\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "In the previous exercise, you have learnt how to transform sentences into vector representations that can be fed to a neural network. Let's use it to do some Sentiment Analysis.\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "Let's first load the ÌMDB dataset: it corresponds to sentences that are movie reviews, each being positive (label 1) or negative (label 0).\n",
    "\n",
    "❓ **Question** ❓ Just load the data. \n",
    "\n",
    "⚠️ **Warning - Reminder** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that a too large number of sentences will make your compute slow down, or even freeze - your RAM can even overflow. For that reason, you can start with 20% of the sentences and see if your computer handles it. Otherwise, rerun with a lower number. On the other hand, you can increase the number if you feel like it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:09:32.440562Z",
     "start_time": "2020-11-20T13:09:18.740517Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    # Load the data\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(sentences_train))\n",
    "        sentences_train = sentences_train[:len_train]\n",
    "        y_train = y_train[:len_train]\n",
    "        \n",
    "        len_test = int(percentage_of_sentences/100*len(sentences_test))\n",
    "        sentences_test = sentences_test[:len_test]\n",
    "        y_test = y_test[:len_test]\n",
    "            \n",
    "    # Load the {interger: word} representation\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    # Convert the list of integers to list of words (str)\n",
    "    X_train = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_train]\n",
    "    X_test = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_test]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "### Just run this cell to load the data\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:10:23.049769Z",
     "start_time": "2020-11-20T13:10:23.044443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Here, let's re-use what you have done in the previous exercise. Reuse the previous function to get data that you can fed to a neural network. To do that, you have to :\n",
    "\n",
    "- Step #1: import gensim and train a word2vec algorithm on the training sentences. You can definitely choose your hyperparameters. But do not load a pretrained model here.\n",
    "- Step #2: convert `X_train` and `X_test` from list of strings (sentences) to list of list of strings (words)\n",
    "- Step #3: convert your list of list of strings to list of list of vectors thanks the trained word2vec embedding.\n",
    "- Step #4: pad your input sequences and store the results in `X_train_pad` and `X_test_pad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:11:07.948072Z",
     "start_time": "2020-11-20T13:11:06.423790Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def convert_string(x):\n",
    "    word_tokens = word_tokenize(x) \n",
    "    text = [w for w in word_tokens ]\n",
    "    return text\n",
    "\n",
    "def convert(X):\n",
    "    output = []\n",
    "    for x in X:\n",
    "        output.append(convert_string(x))\n",
    "    return output\n",
    "\n",
    "sentences_train = convert(X_train)\n",
    "sentences_test = convert(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:13:07.457708Z",
     "start_time": "2020-11-20T13:13:04.742291Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences=sentences_train, size=50, min_count = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:13:50.477380Z",
     "start_time": "2020-11-20T13:13:50.471889Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    output = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv.vocab:\n",
    "            output.append(word2vec.wv[word])\n",
    "        else:\n",
    "            continue\n",
    "    return np.array(output) \n",
    "\n",
    "def embedding(word2vec, sentences):\n",
    "    output = []\n",
    "    for sentence in sentences:\n",
    "        output.append(embed_sentence(word2vec, sentence))\n",
    "    return np.array(output)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:14:10.670827Z",
     "start_time": "2020-11-20T13:14:08.586104Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainf = embedding(word2vec, sentences_train)\n",
    "X_testf = embedding(word2vec, sentences_test)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(X_trainf, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_testf, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To be sure that it worked, please check the following for `X_train_pad` and `X_test_pad` :\n",
    "- they are numpy arrays\n",
    "- they are 3-dimensional\n",
    "- the last dimension is of the size of your word2vec embedding space (you can get it with `word2vec.wv.vector_size`\n",
    "- the first dimension is of the size of your `X_train` and `X_test`\n",
    "\n",
    "✅ **Good Practice** ✅ Such tests are quite important! Not only in this exercise, but in real-life applications. It prevents from searching at errors too late and from letting them propagate through the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:15:31.246579Z",
     "start_time": "2020-11-20T13:15:31.240632Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 50)\n",
    "assert(X_test_pad.shape[2] == 50)\n",
    "type(X_train_pad)\n",
    "assert(X_test_pad.shape[0] == 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "❓ **Question** ❓ What is your baseline accuracy? In this case, your baseline can be to predict the label that is the most present in `y_train` (of course, if the dataset is balanced, the baseline accuracy is 1/n where n is the number of classes - 2 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:17:53.689667Z",
     "start_time": "2020-11-20T13:17:53.683618Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:18:46.598003Z",
     "start_time": "2020-11-20T13:18:46.589071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1282\n",
       "0    1218\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:20:45.575026Z",
     "start_time": "2020-11-20T13:20:45.569061Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.ones(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:21:42.152887Z",
     "start_time": "2020-11-20T13:21:42.144553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy is: 0.4768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "baseline_score = accuracy_score(y_test, y_pred)\n",
    "print('baseline accuracy is:', baseline_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The model\n",
    "\n",
    "❓ **Question** ❓ Write a RNN with the following layers:\n",
    "- a masking layer\n",
    "- a LSTM with 20 units and tanh activation function\n",
    "- a Dense with 10 units\n",
    "- a output layer that depends on your task\n",
    "\n",
    "Then, compile your model (we advise you to use the rmsprop as the optimizer - at least to begin with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:37:41.672309Z",
     "start_time": "2020-11-20T13:37:41.666389Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "test = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:47:33.325859Z",
     "start_time": "2020-11-20T13:47:33.062854Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = init_model()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit the model on your embedded and padded data - do not forget the early stopping criterion.\n",
    "\n",
    "❗ **Remark** ❗ Your accuracy with greatly depend on your training test corpus. Here just make sure that your performance is above the baseline model (which should be the case even if you loaded only 20% of the initial IMDB data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:28:58.126484Z",
     "start_time": "2020-11-20T13:28:58.117970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:33:43.814339Z",
     "start_time": "2020-11-20T13:33:43.806874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = y_train.astype('int32')\n",
    "type(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:53:56.708287Z",
     "start_time": "2020-11-20T13:47:34.635076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "59/59 [==============================] - 81s 1s/step - loss: 0.6904 - accuracy: 0.5149 - val_loss: 0.6917 - val_accuracy: 0.5280\n",
      "Epoch 2/5\n",
      "59/59 [==============================] - 69s 1s/step - loss: 0.6807 - accuracy: 0.5486 - val_loss: 0.6849 - val_accuracy: 0.5533\n",
      "Epoch 3/5\n",
      "59/59 [==============================] - 68s 1s/step - loss: 0.6707 - accuracy: 0.5920 - val_loss: 0.6835 - val_accuracy: 0.5640\n",
      "Epoch 4/5\n",
      "59/59 [==============================] - 68s 1s/step - loss: 0.6616 - accuracy: 0.6091 - val_loss: 0.6754 - val_accuracy: 0.5693\n",
      "Epoch 5/5\n",
      "59/59 [==============================] - 69s 1s/step - loss: 0.6491 - accuracy: 0.6149 - val_loss: 0.6721 - val_accuracy: 0.5853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f32d845bd00>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience = 5)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "          validation_split=0.3,\n",
    "          batch_size=30,\n",
    "          epochs=5,\n",
    "          callbacks=[es],\n",
    "          verbose=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:54:55.435207Z",
     "start_time": "2020-11-20T13:54:40.263886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 130ms/step - loss: 0.6678 - accuracy: 0.5984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6678482890129089, 0.5983999967575073]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Word2Vec - Transfer Learning\n",
    "\n",
    "Your accuracy, while above the baseline model, might be quite low. There are multiple options to improve it, as data cleaning and improving the quality of the embedding.\n",
    "\n",
    "We won't dig into data cleaning strategies here. On the other hand, let's try to improve the quality of our embedding. But instead of just loading a larger corpus, why not benefiting from the embedding that other have learnt? Because, the quality of an embedding, i.e. the proximity of the words, can be derived from different tasks. This is exactly what transfer learning is.\n",
    "\n",
    "❓ **Question** ❓ As shown on the previous exercise, load a pretrained word2vec embedding spave.\n",
    "\n",
    "The list of the different models is available with : \n",
    "\n",
    "```\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))\n",
    "```\n",
    "\n",
    "than you can `api.load(the-model-of-your-choice)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:55:03.072859Z",
     "start_time": "2020-11-20T13:55:02.813889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:55:53.334990Z",
     "start_time": "2020-11-20T13:55:12.913097Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_2 = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Embed `X_train` and `X_test` in your new embedding space (with the new word2vec that you loaded)!\n",
    "Store the results in `X_train_pad_2` and `X_test_pad_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:56:01.905829Z",
     "start_time": "2020-11-20T13:55:53.635144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-637ce17224d5>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in word2vec.wv.vocab:\n",
      "<ipython-input-10-637ce17224d5>:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  output.append(word2vec.wv[word])\n"
     ]
    }
   ],
   "source": [
    "X_trainf = embedding(word2vec_2, sentences_train)\n",
    "X_testf = embedding(word2vec_2, sentences_test)\n",
    "\n",
    "\n",
    "X_train_pad_2 = pad_sequences(X_trainf, dtype='float32', padding='post')\n",
    "X_test_pad_2 = pad_sequences(X_testf, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:56:15.047162Z",
     "start_time": "2020-11-20T13:56:15.038218Z"
    }
   },
   "outputs": [],
   "source": [
    "del X_trainf\n",
    "del X_testf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:56:39.980520Z",
     "start_time": "2020-11-20T13:56:39.975983Z"
    }
   },
   "outputs": [],
   "source": [
    "del X_test_pad\n",
    "del X_train_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Reinitialize a model and fit it on your new embedded (and padded) data!  Evaluate it on your test set and compare it to your previous accuracy.\n",
    "\n",
    "❗ **Remark** ❗ The training could take some time here. You can just compute 10 epochs (this is **not** a good practice, it is just not to wait too long) and go to the next exercise while it trains - or take a break, you probably deserve it ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T13:58:18.542552Z",
     "start_time": "2020-11-20T13:58:18.517968Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = init_model()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T14:05:12.354283Z",
     "start_time": "2020-11-20T13:58:18.970373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "59/59 [==============================] - 79s 1s/step - loss: 0.6951 - accuracy: 0.5069 - val_loss: 0.6936 - val_accuracy: 0.5013\n",
      "Epoch 2/5\n",
      "59/59 [==============================] - 78s 1s/step - loss: 0.6744 - accuracy: 0.5994 - val_loss: 0.6672 - val_accuracy: 0.6267\n",
      "Epoch 3/5\n",
      "59/59 [==============================] - 80s 1s/step - loss: 0.6393 - accuracy: 0.6606 - val_loss: 0.6401 - val_accuracy: 0.6600\n",
      "Epoch 4/5\n",
      "59/59 [==============================] - 83s 1s/step - loss: 0.6346 - accuracy: 0.6509 - val_loss: 0.6580 - val_accuracy: 0.6347\n",
      "Epoch 5/5\n",
      "59/59 [==============================] - 83s 1s/step - loss: 0.6337 - accuracy: 0.6371 - val_loss: 0.6542 - val_accuracy: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f33839fe940>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience = 5)\n",
    "\n",
    "model.fit(X_train_pad_2, y_train,\n",
    "          validation_split=0.3,\n",
    "          batch_size=30,\n",
    "          epochs=5,\n",
    "          callbacks=[es],\n",
    "          verbose=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ According to you, what causes the model to take so much time to train, especially compared to the first training? To understand it, you can check the size of `X_train_pad` compared to `X_train_pad_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T14:06:15.945057Z",
     "start_time": "2020-11-20T14:06:01.955236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 130ms/step - loss: 0.6110 - accuracy: 0.6964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6110473871231079, 0.696399986743927]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad_2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T14:06:23.725593Z",
     "start_time": "2020-11-20T14:06:23.717356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1635, 25)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because your new word2vec has been trained on a large corpus, it has a representation for many many words! Way more than with your small dataset, especially as you discarder words that were not present more than a given number of time in the train set. For that reason, you have way more embedded words in your train and test set, which makes each iteration longer than previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "244.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
