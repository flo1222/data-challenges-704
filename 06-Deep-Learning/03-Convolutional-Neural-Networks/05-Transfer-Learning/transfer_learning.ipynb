{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp81GSFN7uHY"
      },
      "source": [
        "# Deep Learning week - Day 3 - Transfer Learning\n",
        "\n",
        "### Exercise objetives\n",
        "- Get familiar with Google Colab\n",
        "- Use a pretrained neural network : Transfer learning\n",
        "\n",
        "<hr>\n",
        "<hr>\n",
        "\n",
        "\n",
        "# Google Colab\n",
        "\n",
        "Once again, use Google Colab to run the following notebook. Do not forget to set the runtime type to GPU.\n",
        "\n",
        "\n",
        "# The exercise\n",
        "\n",
        "\n",
        "This notebook is dedicated to **transfer learning**. \n",
        "\n",
        "We have seen that the convolutions are mathematical operation that detect specific patterns in input images and use them to classify the image. One could imagine that these patterns are not 100% specific to the task but to the input images. Therefore, why not using convolutions that have been learnt on other task with the expectation that it will also work in other scenario. This has two advantages: taking less time to train and benefiting from complex architecture that have been trained for state-of-the-art challenges. We here _transfer_ a CNN from one task to another => _transfer learning_. \n",
        "\n",
        "\n",
        "⚠️ The convolutions may not be specific! However, the last layer is by design specific to the problem it was trained on! Therefore, this last layer is usually removed, replace by a layer that is design to the task. As this new last layer has random weight, it has to be retrained. This is called _fine-tunning_. \n",
        "\n",
        "\n",
        "In this exercise, we will use the [VGG-16 Neural Network](https://neurohive.io/en/popular-networks/vgg16/), a well-known architecture that has been trained on ImageNet which is a very large database of images of different categories. In a nutshell, this architecture has already learnt kernels which are supposed to be good not only for the task it has been train on but maybe for other tasks. \n",
        "\n",
        "The idea is that first layers are not specialized for the particular task it has been trained on ; only the last ones are. Therefore, we will load the existing VGG16 network, remove the last fully connected layers, replace them by new connected layers (whose weights are randomly set), and train these last layers on a specific classification task - here, separate types of flower. The underlying idea is that the first convolutional layers of VGG-16, that has already been trained, corresponds to filters that are able to extract meaning features from images. And you will only learn the last layers for your particular problem.\n",
        "\n",
        "\n",
        "# Data loading & Preprocessing\n",
        "\n",
        "Here, we will load the same data as in the previous exercise and try to improve our previous performance.\n",
        "\n",
        "❓ **Question** ❓ As in the previous exercise, load the flower picture data and normalize them. You can get back to the previous exercise to get the usefull links and functions.\n",
        "\n",
        "⚠️ **Warning** ⚠️ DO NOT NORMALIZE THE DATA! You will see later why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTfAmFyt-HHr",
        "outputId": "3b5862c7-1717-4171-d3b8-790a86453ab1"
      },
      "source": [
        "!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-18 15:28:29--  https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
            "Resolving wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)... 52.218.97.42\n",
            "Connecting to wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)|52.218.97.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104983809 (100M) [application/zip]\n",
            "Saving to: ‘flowers-dataset.zip’\n",
            "\n",
            "flowers-dataset.zip 100%[===================>] 100.12M  28.6MB/s    in 3.5s    \n",
            "\n",
            "2020-11-18 15:28:33 (28.6 MB/s) - ‘flowers-dataset.zip’ saved [104983809/104983809]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pos2BFKX-M2o"
      },
      "source": [
        "!unzip flowers-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs0Oz31W92xS"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_flowers_data(loading_method):\n",
        "    if loading_method == 'colab':\n",
        "        data_path = '/content/drive/My Drive/Deep_learning_data/flowers'\n",
        "    elif loading_method == 'direct':\n",
        "        data_path = 'flowers/'\n",
        "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for (cl, i) in classes.items():\n",
        "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
        "        for img in tqdm(images_path[:300]):\n",
        "            path = os.path.join(data_path, cl, img)\n",
        "            if os.path.exists(path):\n",
        "                image = Image.open(path)\n",
        "                image = image.resize((256, 256))\n",
        "                imgs.append(np.array(image))\n",
        "                labels.append(i)\n",
        "\n",
        "    X = np.array(imgs)\n",
        "    num_classes = len(set(labels))\n",
        "    y = to_categorical(labels, num_classes)\n",
        "\n",
        "    # Finally we shuffle:\n",
        "    p = np.random.permutation(len(X))\n",
        "    X, y = X[p], y[p]\n",
        "\n",
        "    first_split = int(len(imgs) /6.)\n",
        "    second_split = first_split + int(len(imgs) * 0.2)\n",
        "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
        "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV1vuuJe-S_A",
        "outputId": "8c24117f-b83b-41bb-e301-14975c0e275b"
      },
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('direct')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [00:00<00:00, 301.07it/s]\n",
            "100%|██████████| 300/300 [00:01<00:00, 282.37it/s]\n",
            "100%|██████████| 299/299 [00:01<00:00, 263.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6m5eKs9s54"
      },
      "source": [
        "# Transfer learning: VGG16 model\n",
        "\n",
        "Let's now build our model. \n",
        "\n",
        "❓ **Question** ❓ Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Especially, look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) to load the model where:\n",
        "- the `weights` have been learnt on `imagenet`\n",
        "- the `input_shape` corresponds to the input shape of any of your images - you have to resize them in case they are not of the same size\n",
        "- the `include_top` argument is set to `False` in order not to load the fully-connected layers of the VGG-16 without the last layer which was specifically trained on `imagenet`\n",
        "\n",
        "❗ **Remark** ❗ Do not change the default value of the other arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfGp3OUC-UOp",
        "outputId": "a5489cfc-622b-46dd-92c0-496b34a084de"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(571, 256, 256, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CycktgCV92xT",
        "outputId": "ddae21b6-4b7b-4866-a5ce-ae758643ffc8"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def load_model():\n",
        "    \n",
        "    model = VGG16(\n",
        "      include_top=False, input_shape=(256, 256, 3)\n",
        "      )\n",
        "    \n",
        "    return model\n",
        "model = load_model()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpMtCElk92xT"
      },
      "source": [
        "❓ **Question** ❓ Look at the architecture of the model thanks to the summary method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4oj9zDR92xT",
        "outputId": "a84a3202-c1e8-492f-8c71-c59c402a75cf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn-HKfBg92xT"
      },
      "source": [
        "Impressive, right? Two things to notice:\n",
        "- It ends with a convolution layer (namely a maxpooling layer that is the layer that follows a convolution). The flattening of the output and the fully connected layers are not here yet! We need to add them !\n",
        "- There are more than 14.000.000 parameters, which is a lot. We could fine-tune them, meaning update them as we will update the last layers weights, but it will take a lot of time. For that reason, we will inform the model that the layers until the flattening are non-trainable.\n",
        "\n",
        "❓ **Question** ❓ Write a first function that takes the previous model as input the set the girst layers to be non-trainable, by applying `model.trainable = False`. Then check-out the summary of the model to see that now, the parameters are `non-trainable`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN3JWwQv92xT",
        "outputId": "1e9f8348-3d0f-4c87-8f8d-05ec66131b64"
      },
      "source": [
        "def set_nontrainable_layers(model):\n",
        "    model.trainable = False\n",
        "    return model\n",
        "    \n",
        "model = set_nontrainable_layers(model)\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9VczA3B92xT"
      },
      "source": [
        "❓ **Question** ❓ We will write a function that adds flattening and dense layers after the first convolutional layers. To do so, cannot directly use the classic `layers.Sequential()` instantiation.\n",
        "\n",
        "For that reason, we will see another one here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. See this example : \n",
        "\n",
        "\n",
        "```\n",
        "base_model = load_model()\n",
        "base_model = set_nontrainable_layers(base_model):\n",
        "flattening_layer = layers.Flatten()\n",
        "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
        "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  flattening_layer,\n",
        "  dense_layer,\n",
        "  prediction_layer\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "The first line loads a group of layer which is the previous VGG-16 model. Then, we set this layers to be non-tranable. Then, we can instantiate as many layers as we want.\n",
        "\n",
        "Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network. \n",
        "\n",
        "Replicate the following steps by adding a flattening and two dense layers (the first with 500 neurons) to the previous VGG-16 model (do not forget to set the layers to be non-trainable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ioHxN-y92xT"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "def add_last_layers():\n",
        "    base_model = load_model()\n",
        "    base_model = set_nontrainable_layers(base_model)\n",
        "    flattening_layer = layers.Flatten()\n",
        "    dense_layer = layers.Dense(500, activation='relu')\n",
        "    dense_layer2 = layers.Dense(100, activation='relu')\n",
        "    prediction_layer = layers.Dense(3, activation='softmax')\n",
        "    \n",
        "    model = Sequential([\n",
        "      base_model,\n",
        "      flattening_layer,\n",
        "      dense_layer,\n",
        "      dense_layer2,\n",
        "      prediction_layer\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKbZkJ-MCRSX"
      },
      "source": [
        "\n",
        "model = add_last_layers()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWrXef5F92xU"
      },
      "source": [
        "❓ **Question** ❓ Now look at the layers and parameters of your model. Note that there is a distinction, at the end, between the trainable and non-trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr5A42PK92xU",
        "outputId": "a6dab401-3d47-4245-f130-11cae654c0ce"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 8, 8, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               16384500  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 31,149,591\n",
            "Trainable params: 16,434,903\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJujuH8t92xU"
      },
      "source": [
        "❓ **Question** ❓ Write a function to compile your model - we advise Adam with `learning_rate=1e-4`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3NX81FX92xU"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "adam = Adam(learning_rate=1e-4)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBgKUuq8EJot"
      },
      "source": [
        "def compile(model):\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI0sHMwe92xU"
      },
      "source": [
        "❓ **Question** ❓ Write an overall function that :\n",
        "- loads the model\n",
        "- updates the layers\n",
        "- compiles it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l5LvRej92xU"
      },
      "source": [
        "\n",
        "def build_model():\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjnLMoA92xU"
      },
      "source": [
        "# Back to the data\n",
        "\n",
        "The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did not normalized them earlier.\n",
        "\n",
        "❓ **Question** ❓ Apply this processing to the images here using the method `preprocess_input` that you can import from `tensorflow.keras.applications.vgg16`. See [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B--Gyb-23YDb"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2H0KZF-EoI"
      },
      "source": [
        "# Run the model\n",
        "\n",
        "❓ **Question** ❓ Now estimate the model, with an early stopping criterion on the validation accuracy - here, the validation data are provided, therefore use `validation_data` instead of `validation_split`.\n",
        "\n",
        "❗ **Remark** ❗ Store the results in a `history` variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z97kx9yUAas5"
      },
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_I9JpiAm-W"
      },
      "source": [
        "❓ **Question** ❓ Plot the accuracy for the test and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCi94aKd92xU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3plexlQAtcC"
      },
      "source": [
        "❓ **Question** ❓ Evaluate the model accuracy on the test set. What is the chance level on this classification task (i.e. accuracy of a random classifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzU0wCXlB6UI"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzetiM3XA2fu"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "The next question are a less guided as they directly derive from what you have done in the previous exercise - don't hesitate to come back to what you have done.\n",
        "\n",
        "❓ **Question** ❓ Use some data augmentation techniques for this task - you can store the fitting in a `history_data_aug` variable that you can plot. Do you see an improvement ? Don't forget to evaluate it on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OuZhaqu92xU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF39HIb7BSOy"
      },
      "source": [
        "# Improve the model\n",
        "\n",
        "You can here try to improve the model test accuracy. To do that, here are some options you can consider\n",
        "\n",
        "1) Is my model overfitting ? If yes, you can try more data augmentation. If no, try a more complex model (unlikely the case here)\n",
        "\n",
        "2) Perform precise grid search on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
        "\n",
        "3) Change the base model to more modern one (resnet, efficient nets) available in the keras library\n",
        "\n",
        "4) Curate the data: maintaining a sane data set is one of the keys to success.\n",
        "\n",
        "5) Obtain more data\n",
        "\n",
        "\n",
        "❗ **Remark** ❗ Note also that it is good practice to perform a real cross-validation. You can also try to do that here to be sure of your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IyqGWzGBN0Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}