{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, you'll try to predict the severity of car accidents, based on features collected from after-crash police investigation\n",
    "\n",
    "This [Kaggle challenge](https://www.kaggle.com/c/accident-severity) comprises of 1,000,000 accidents report, split into multiple `.csv` files.\n",
    "\n",
    "**The goal of the model is to predict the severity of car accidents**. The target variable is called `grav` (for 'gravity') in the file `users.csv`. This variable has four levels, but in this challenge, we'll convert it to a binary classification problem. We will:\n",
    "- Load data into pandas\n",
    "- Create a single DataFrame for our problem, where each row is a user involved in an accident\n",
    "- Extract the features you think would be relevant to predict its severity\n",
    "- Build a data pipeline that gives you a baseline model\n",
    "- Then, iterate on the different phase and try to get the best model! \n",
    "\n",
    "üî• **Today is a special challenge** :\n",
    "- You will send your best score to your batch slack channel!\n",
    "- The winner will present its notebook to the class during the recap session at 5pm üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "‚ö†Ô∏è **Good practices to follow for large exploratory notebooks**\n",
    "- Build your Notebook linearily so that it can always be run from top to bottom without any errors\n",
    "- Clean the outputs of your cells that are not needed\n",
    "- Clean your variables in memory when you don't need them (especially when they are very large). You can use the python built-in function `del`, or the the **Jupyter nbextentions** `variable_inspector`\n",
    "- Make heavy use of `table_of_content` and `collapsable_headings` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sourcing\n",
    "\n",
    "Let's get started! The data we want to use is from the `csv` files in `/data/data_training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florent/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "cara = pd.read_csv(\"data/data_training/caracteristics.csv\", encoding=\"ISO-8859-1\")\n",
    "users = pd.read_csv(\"data/data_training/users.csv\", encoding=\"ISO-8859-1\")\n",
    "places = pd.read_csv(\"data/data_training/places.csv\", encoding=\"ISO-8859-1\")\n",
    "vehicles = pd.read_csv(\"data/data_training/vehicles.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Explore the different tables, and the different variables using `challenge_variable.md`, which provides a description of features. More details can be found [here](https://www.data.gouv.fr/fr/datasets/r/8d4df329-bbbb-434c-9f1f-596d78ad529f) if needed, or in the [Kaggle](https://www.kaggle.com/ahmedlahlou/accidents-in-france-from-2005-to-2016/discussion) discussion channel. Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We will create one single dataset where each row should represent a `user` in a car, by merging the data from the different files dataset.  \n",
    "**Take some time to think about how you would do it yourself**, and only then, read carefully through the code below to understand exactly what we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge caracteristics and places on 'Num_Acc'\n",
    "data = cara.merge(places, on='Num_Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a common key to merge users amd vehicles on\n",
    "users['Num_Acc_num_veh'] = users['Num_Acc'].map(lambda x: str(x)) + users['num_veh']\n",
    "vehicles['Num_Acc_num_veh'] = vehicles['Num_Acc'].map(lambda x: str(x)) + vehicles['num_veh']\n",
    "# Remove useless columns\n",
    "vehicles = vehicles.drop(columns=['index'])\n",
    "users = users.drop(columns=['index', 'Num_Acc', 'num_veh'])\n",
    "# Merge vehicles and users\n",
    "tmp = vehicles.merge(users, on='Num_Acc_num_veh', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on 'Num_Acc'\n",
    "data = data.merge(tmp, on='Num_Acc', how='inner')\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cara\n",
    "del places\n",
    "del users\n",
    "del vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We will apply some preprocessing methods like standardization or missing values removal or imputing.\n",
    "Remember to look at `challenge_variable.md` for a description of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lines without targets (if any)\n",
    "data_cleaned = data[~np.isnan(data.grav)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v2                 0.953958\n",
       "lat                0.632648\n",
       "long               0.632648\n",
       "gps                0.631134\n",
       "pr1                0.463472\n",
       "pr                 0.462609\n",
       "v1                 0.383683\n",
       "adr                0.184124\n",
       "voie               0.048149\n",
       "place              0.032898\n",
       "secu               0.011189\n",
       "lartpc             0.003175\n",
       "larrout            0.002416\n",
       "an_nais            0.001841\n",
       "nbv                0.001731\n",
       "vosp               0.001060\n",
       "actp               0.000882\n",
       "locp               0.000867\n",
       "etatp              0.000825\n",
       "infra              0.000662\n",
       "env1               0.000653\n",
       "plan               0.000445\n",
       "prof               0.000443\n",
       "surf               0.000427\n",
       "obs                0.000427\n",
       "circ               0.000392\n",
       "situ               0.000355\n",
       "obsm               0.000236\n",
       "trajet             0.000186\n",
       "manv               0.000106\n",
       "choc               0.000064\n",
       "atm                0.000039\n",
       "col                0.000017\n",
       "com                0.000005\n",
       "catr               0.000002\n",
       "hrmn               0.000000\n",
       "lum                0.000000\n",
       "agg                0.000000\n",
       "int                0.000000\n",
       "jour               0.000000\n",
       "mois               0.000000\n",
       "an                 0.000000\n",
       "Num_Acc            0.000000\n",
       "occutc             0.000000\n",
       "sexe               0.000000\n",
       "dep                0.000000\n",
       "index_y            0.000000\n",
       "catv               0.000000\n",
       "grav               0.000000\n",
       "catu               0.000000\n",
       "Num_Acc_num_veh    0.000000\n",
       "num_veh            0.000000\n",
       "senc               0.000000\n",
       "index_x            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whih features with highest ratio of NaN per column\n",
    "(data_cleaned.isna().sum() / data_cleaned.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove too incomplete features\n",
    "too_incomplete_features=[\n",
    "    'locp', 'actp', 'etatp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that can be safely considered useless for the predictive power of our model\n",
    "useless_features=[\n",
    "    'v2', 'lat', 'long', 'gps', 'pr1', 'pr', 'v1', 'adr', 'voie',\n",
    "    'index_x', 'Num_Acc', 'Num_Acc_num_veh', 'Num_Acc', 'num_veh', 'index_y',\n",
    "    'jour', 'an',\n",
    "    'dep', 'com', 'env1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.drop(columns=too_incomplete_features+useless_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1209362, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160867, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del useless_features\n",
    "del too_incomplete_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `data_cleaned` dataset! Let's now engineer our features as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numerical = ['lartpc', 'larrout', 'occutc', 'an_nais', 'nbv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "def preprocess_numerical_features(X):\n",
    "    '''\n",
    "    Returns a new DataFrame with\n",
    "    - Missing values replaced by Column Mean\n",
    "    - Features Standard Scaled\n",
    "    - Original Features names kept in the DataFrame\n",
    "    '''\n",
    "    data_num = pd.DataFrame.copy(X)\n",
    "    imputer = SimpleImputer(strategy = 'mean')\n",
    "    data_num[features_numerical] = imputer.fit_transform(data_num[features_numerical])\n",
    "    scaler = StandardScaler()\n",
    "    data_num[features_numerical] = scaler.fit_transform(data_num[features_numerical])\n",
    "    return data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lartpc</th>\n",
       "      <th>larrout</th>\n",
       "      <th>occutc</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>nbv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.495102</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>1.273883</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.495102</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-0.224675</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-0.835199</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.279217</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.168212</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209357</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>1.333487</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>0.052836</td>\n",
       "      <td>-1.248930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209358</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>1.333487</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>1.051875</td>\n",
       "      <td>-1.248930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209359</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>1.333487</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-0.335680</td>\n",
       "      <td>-1.248930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209360</th>\n",
       "      <td>0.659032</td>\n",
       "      <td>0.171249</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.112710</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209361</th>\n",
       "      <td>0.659032</td>\n",
       "      <td>0.171249</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.390221</td>\n",
       "      <td>-0.052801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209362 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lartpc   larrout  occutc   an_nais       nbv\n",
       "0       -0.246173 -0.495102 -0.0581  1.273883 -0.052801\n",
       "1       -0.246173 -0.495102 -0.0581 -0.224675 -0.052801\n",
       "2       -0.246173 -0.138682 -0.0581 -0.835199 -0.052801\n",
       "3       -0.246173 -0.138682 -0.0581 -1.279217 -0.052801\n",
       "4       -0.246173 -0.138682 -0.0581 -1.168212 -0.052801\n",
       "...           ...       ...     ...       ...       ...\n",
       "1209357 -0.246173  1.333487 -0.0581  0.052836 -1.248930\n",
       "1209358 -0.246173  1.333487 -0.0581  1.051875 -1.248930\n",
       "1209359 -0.246173  1.333487 -0.0581 -0.335680 -1.248930\n",
       "1209360  0.659032  0.171249 -0.0581 -1.112710 -0.052801\n",
       "1209361  0.659032  0.171249 -0.0581 -1.390221 -0.052801\n",
       "\n",
       "[1209362 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your code below\n",
    "preprocess_numerical_features(data_cleaned[features_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you get a Warning \"A value is trying to be set on a copy of a slice from a DataFrame\"?\n",
    "If so, it may be because you are trying to modify the input DataFrame `data_cleaned`!\n",
    "\n",
    "Read this [important blog on copy vs. view](https://www.practicaldatascience.org/html/views_and_copies_in_pandas.html) of pandas DataFrame and try to solve your warning by yourself\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "`pd.DataFrame.copy()`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you see any cyclical features to process specifically?\n",
    "# This can be done after a first baseline model is created.\n",
    "features_cyclical = ['hrmn','mois']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def preprocess_cyclical_features(X):\n",
    "    '''\n",
    "    Input: DataFrame X\n",
    "    Output: Returns new DataFrame, where all its features X_i have been replaced\n",
    "    by both their sin(2*Pi / cycle_length * X_i) and cos(2*Pi / cycle_length * X_i), and delete initial feature X_i.\n",
    "    '''\n",
    "    list_cols = X.columns\n",
    "    new_X = X.copy()\n",
    "    for feature, cycle_length in zip(features_cyclical,[12,30]):\n",
    "        new_X[feature+'_sin'] = X[feature].map(lambda x: math.sin(2*math.pi / (x * cycle_length)))\n",
    "        new_X[feature+'_cos'] = X[feature].map(lambda x: math.cos(2*math.pi / (x * cycle_length)))\n",
    "    new_X.drop(columns=list_cols, inplace=True)\n",
    "    return pd.DataFrame(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hrmn_sin</th>\n",
       "      <th>hrmn_cos</th>\n",
       "      <th>mois_sin</th>\n",
       "      <th>mois_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.019039</td>\n",
       "      <td>0.999819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.019039</td>\n",
       "      <td>0.999819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209357</th>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209358</th>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209359</th>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209360</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209361</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209362 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         hrmn_sin  hrmn_cos  mois_sin  mois_cos\n",
       "0        0.004028  0.999992  0.019039  0.999819\n",
       "1        0.004028  0.999992  0.019039  0.999819\n",
       "2        0.000457  1.000000  0.207912  0.978148\n",
       "3        0.000457  1.000000  0.207912  0.978148\n",
       "4        0.000457  1.000000  0.207912  0.978148\n",
       "...           ...       ...       ...       ...\n",
       "1209357  0.000349  1.000000  0.017452  0.999848\n",
       "1209358  0.000349  1.000000  0.017452  0.999848\n",
       "1209359  0.000349  1.000000  0.017452  0.999848\n",
       "1209360  0.000249  1.000000  0.017452  0.999848\n",
       "1209361  0.000249  1.000000  0.017452  0.999848\n",
       "\n",
       "[1209362 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_cyclical_features(data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the last group of feature (categorical features) without hardcoding them manually. Then, create the associated preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical = list(set(data_cleaned.columns) - set(features_numerical) - set(features_cyclical) - {'grav'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categorical_features(X):   \n",
    "    ''' Returns a new DataFrame with dummified columns'''\n",
    "    tmp = pd.DataFrame.copy(X.dropna().astype(int))\n",
    "    tmp = pd.get_dummies(tmp, columns = features_categorical)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def preprocess_categorical_features2(X):   \n",
    "    ''' Returns a new DataFrame with dummified columns'''\n",
    "    tmp = pd.DataFrame.copy(X.dropna().astype(int))\n",
    "    oht = OneHotEncoder(handle_unknown='ignore')\n",
    "    tmp = oht.fit_transform(tmp[features_categorical])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obsm_0</th>\n",
       "      <th>obsm_1</th>\n",
       "      <th>obsm_2</th>\n",
       "      <th>obsm_4</th>\n",
       "      <th>obsm_5</th>\n",
       "      <th>obsm_6</th>\n",
       "      <th>obsm_9</th>\n",
       "      <th>int_0</th>\n",
       "      <th>int_1</th>\n",
       "      <th>int_2</th>\n",
       "      <th>...</th>\n",
       "      <th>surf_0</th>\n",
       "      <th>surf_1</th>\n",
       "      <th>surf_2</th>\n",
       "      <th>surf_3</th>\n",
       "      <th>surf_4</th>\n",
       "      <th>surf_5</th>\n",
       "      <th>surf_6</th>\n",
       "      <th>surf_7</th>\n",
       "      <th>surf_8</th>\n",
       "      <th>surf_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209358</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209359</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209360</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209361</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166887 rows √ó 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         obsm_0  obsm_1  obsm_2  obsm_4  obsm_5  obsm_6  obsm_9  int_0  int_1  \\\n",
       "0             1       0       0       0       0       0       0      0      1   \n",
       "1             1       0       0       0       0       0       0      0      1   \n",
       "2             0       0       1       0       0       0       0      0      0   \n",
       "3             0       0       1       0       0       0       0      0      0   \n",
       "4             0       0       1       0       0       0       0      0      0   \n",
       "...         ...     ...     ...     ...     ...     ...     ...    ...    ...   \n",
       "1209357       0       0       1       0       0       0       0      0      0   \n",
       "1209358       0       0       1       0       0       0       0      0      0   \n",
       "1209359       0       0       1       0       0       0       0      0      0   \n",
       "1209360       1       0       0       0       0       0       0      0      1   \n",
       "1209361       1       0       0       0       0       0       0      0      1   \n",
       "\n",
       "         int_2  ...  surf_0  surf_1  surf_2  surf_3  surf_4  surf_5  surf_6  \\\n",
       "0            0  ...       0       1       0       0       0       0       0   \n",
       "1            0  ...       0       1       0       0       0       0       0   \n",
       "2            1  ...       0       1       0       0       0       0       0   \n",
       "3            1  ...       0       1       0       0       0       0       0   \n",
       "4            1  ...       0       1       0       0       0       0       0   \n",
       "...        ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1209357      0  ...       0       1       0       0       0       0       0   \n",
       "1209358      0  ...       0       1       0       0       0       0       0   \n",
       "1209359      0  ...       0       1       0       0       0       0       0   \n",
       "1209360      0  ...       0       0       1       0       0       0       0   \n",
       "1209361      0  ...       0       0       1       0       0       0       0   \n",
       "\n",
       "         surf_7  surf_8  surf_9  \n",
       "0             0       0       0  \n",
       "1             0       0       0  \n",
       "2             0       0       0  \n",
       "3             0       0       0  \n",
       "4             0       0       0  \n",
       "...         ...     ...     ...  \n",
       "1209357       0       0       0  \n",
       "1209358       0       0       0  \n",
       "1209359       0       0       0  \n",
       "1209360       0       0       0  \n",
       "1209361       0       0       0  \n",
       "\n",
       "[1166887 rows x 225 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_categorical_features(data_cleaned[features_categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the new `data_preprocessed` dataset by concatenating all three preprocessing, and then drop all remaining NaN that could not have been handled previously despite our preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = pd.merge(preprocess_numerical_features(data_cleaned[features_numerical]),\\\n",
    "                             preprocess_categorical_features(data_cleaned[features_categorical]),\\\n",
    "                             left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lartpc</th>\n",
       "      <th>larrout</th>\n",
       "      <th>occutc</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>nbv</th>\n",
       "      <th>obsm_0</th>\n",
       "      <th>obsm_1</th>\n",
       "      <th>obsm_2</th>\n",
       "      <th>obsm_4</th>\n",
       "      <th>obsm_5</th>\n",
       "      <th>...</th>\n",
       "      <th>surf_0</th>\n",
       "      <th>surf_1</th>\n",
       "      <th>surf_2</th>\n",
       "      <th>surf_3</th>\n",
       "      <th>surf_4</th>\n",
       "      <th>surf_5</th>\n",
       "      <th>surf_6</th>\n",
       "      <th>surf_7</th>\n",
       "      <th>surf_8</th>\n",
       "      <th>surf_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.495102</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>1.273883</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.495102</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-0.224675</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-0.835199</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.279217</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.246173</td>\n",
       "      <td>-0.138682</td>\n",
       "      <td>-0.0581</td>\n",
       "      <td>-1.168212</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lartpc   larrout  occutc   an_nais       nbv  obsm_0  obsm_1  obsm_2  \\\n",
       "0 -0.246173 -0.495102 -0.0581  1.273883 -0.052801       1       0       0   \n",
       "1 -0.246173 -0.495102 -0.0581 -0.224675 -0.052801       1       0       0   \n",
       "2 -0.246173 -0.138682 -0.0581 -0.835199 -0.052801       0       0       1   \n",
       "3 -0.246173 -0.138682 -0.0581 -1.279217 -0.052801       0       0       1   \n",
       "4 -0.246173 -0.138682 -0.0581 -1.168212 -0.052801       0       0       1   \n",
       "\n",
       "   obsm_4  obsm_5  ...  surf_0  surf_1  surf_2  surf_3  surf_4  surf_5  \\\n",
       "0       0       0  ...       0       1       0       0       0       0   \n",
       "1       0       0  ...       0       1       0       0       0       0   \n",
       "2       0       0  ...       0       1       0       0       0       0   \n",
       "3       0       0  ...       0       1       0       0       0       0   \n",
       "4       0       0  ...       0       1       0       0       0       0   \n",
       "\n",
       "   surf_6  surf_7  surf_8  surf_9  \n",
       "0       0       0       0       0  \n",
       "1       0       0       0       0  \n",
       "2       0       0       0       0  \n",
       "3       0       0       0       0  \n",
       "4       0       0       0       0  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = pd.merge(preprocess_cyclical_features(data_cleaned),data_preprocessed,\\\n",
    "                             left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1166887, 234)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = pd.merge(data_cleaned[['grav']],data_preprocessed,\\\n",
    "                             left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1166887, 235)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "‚ùì Create X and y, and don't forget to convert the classification into a binary task.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "data['grav_binary'] = data['grav'].replace({1: 0, 4: 0, 2: 1, 3: 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 235)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed_s = data_preprocessed.sample(10000)\n",
    "data_preprocessed_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1166887,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = data_preprocessed.drop(columns = ['grav'])\n",
    "y = data_preprocessed['grav'].replace({1: 0, 4: 0, 2: 1, 3: 1})\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = data_preprocessed_s.drop(columns = ['grav'])\n",
    "y_small = data_preprocessed_s['grav'].replace({1: 0, 4: 0, 2: 1, 3: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split both datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_small, y_small, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) Create here a train/eval split within the train set itself.\n",
    "# Some powerfull models (XGBOOST, Neural Network...) which are prone to overfitting on the traning set, needs \"early stopping criteria\", to avoid descending the gradient completely and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a dataset ready for training! \n",
    "**Skip directly to section 5 to get a baseline model working ASAP**, and only then come back to this section 4 if you want to better understand your X and get inspiration for the best model to use, or for some feature selection to reduce model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìInvestigate your X. Are features strongly correlated? Are some feature more important than other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_preprocessed.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìFit a PCA and plot the cumulated sum of explained variance ratio of your Principal Component. Do you see any clear elbow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_small, y_small)\n",
    "var = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "kwargs = {'cumulative': True}\n",
    "sns.distplot(var, hist_kws=kwargs, kde_kws=kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest-based most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fit a default RandomForestClassifier on a small smaple to estimate the top 20 feature importance. Do they make intuitive sense to your point of view?  Do you see any clear elbow for dimension-reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "forest_cv = cross_validate(forest, \n",
    "                            X_small,\n",
    "                            y_small,\n",
    "                            scoring = \"f1_macro\",\n",
    "                            cv=5)\n",
    "\n",
    "forest_cv['test_score']\n",
    "forest.fit(Xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['an_nais', 'hrmn_sin', 'larrout', 'mois_sin', 'mois_cos', 'nbv',\n",
       "       'secu_11', 'obs_0', 'catr_3', 'agg_2', 'secu_21', 'catv_7',\n",
       "       'lartpc', 'agg_1', 'catr_4', 'choc_1', 'trajet_5', 'manv_1',\n",
       "       'circ_2', 'col_1'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_results = pd.DataFrame({'feature': X_small.columns,'importance': forest.feature_importances_})\n",
    "forest_results.sort_values(by='importance', ascending = False).head(20)\n",
    "top20 = forest_results.sort_values(by='importance', ascending = False).head(20)\n",
    "top20features = top20['feature'].values\n",
    "top20features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top20 = data_preprocessed[top20features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì (Optional) There are better ways to estimate feature importance in a RandomForest. Feel free to try to two following options\n",
    "\n",
    "**Option 1** : Recursive-method\n",
    "1. Train a first model, note top1 feature (computed based on the gini-explicative power of the feature, in each tree)\n",
    "2. Remove top1 from your X and retrain a RandomForest. Note top1 feature and it's relative importance\n",
    "3. Loop\n",
    "\n",
    "**Option 2** : Permutation-method ([sklearn.inspection.permutation_importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)), works with any model!\n",
    "1. Train a first model, keep track of its accuracy\n",
    "2. Take one feature and shuffle its columns. Compute new accuracy of the corrupted dataset, and note by how much it has been reduced.\n",
    "3. Loop over all features and rank them by accuracy reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the class balance of your target?  \n",
    "Do you think acccuracy would be a good score?\n",
    "If you don't want to favor any class over the other, what would be a good performance metric for your problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = y.value_counts()[1]/(y.value_counts()[1] + y.value_counts()[0])\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dum = DummyClassifier(strategy='most_frequent')\n",
    "dum.fit(Xs_train, ys_train)\n",
    "dum.score(Xs_test, ys_test)\n",
    "print(classification_report(ys_test, dum.predict(Xs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(dum, X_small, y_small, scoring = 'f1_macro', cv = 5)\n",
    "print(cv_results[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "In such an unbalanced problem, accuracy is meaningless: A very dumb model predicting always zeros would have great accuracy, to the detriment of the predictive power of class  1, which has precision and recall equal to zero!\n",
    "    \n",
    "The non-weighted mean between both f1 score of each class called `f1_macro` would be a good measure for this type of problem.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model (A first iteration)\n",
    "\n",
    "‚ùì Create a simple model, fast to train, to classify the severity of the accidents. Start simple. Don't forget to fit on your training set and evaluate the score on your test set. Can you beat the Baseline? What about its `f1_macro` score? Measure the time it takes on the full dataset, with `%%time` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# log = LogisticRegression(max_iter = 1000)\n",
    "# cv_results = cross_validate(log, X_small, y_small, scoring = 'f1_macro', cv = 5)\n",
    "# print(cv_results[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.neighbors import KNeighborsClassifier \n",
    "# from sklearn.model_selection import cross_validate\n",
    "# knn = KNeighborsClassifier()\n",
    "# cv_results = cross_validate(knn, X_small, y_small, scoring = 'f1_macro', cv = 5)\n",
    "# print(cv_results[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî•üî•üî• Advanced Models - LeWagon batch contest ! üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now it's your turn to shine! Play with different models and try to find the best one on your training set!\n",
    "- Send your best `f1_macro` score (as defined above) to your slack channel without saying which model you used!\n",
    "- ‚ö†Ô∏è Only send score tested on a full `y_test` of complete size (30% of 1M rows!) will be taken into account\n",
    "- Feel free to use your X_small for investigation purpose\n",
    "- If it takes too long to train, simplify your model, or use better feature preprocessing/selection\n",
    "\n",
    "The winner will present its notebook to the class during the reboot üí™\n",
    "\n",
    "(Don't forget, your Notebook should be made to be run from top to bottom in one go!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Some hints</summary>\n",
    "Take a closer look at feature engineering: Are there some features we haven't correctly preprocessed?  \n",
    "    \n",
    "Most of the time, a good dataset trumps a good model!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_preprocessed\n",
    "del data_preprocessed_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98 ¬µs, sys: 0 ns, total: 98 ¬µs\n",
      "Wall time: 106 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "log = LogisticRegression(max_iter = 200, C=3, class_weight = 'balanced')\n",
    "cv_results = cross_validate(log, data_top20, y, scoring = 'f1_macro', cv = 3, n_jobs = -1)\n",
    "print(cv_results[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 242\u001b[0;31m     scores = parallel(\n\u001b[0m\u001b[1;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/envs/lewagon/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "ensemble = VotingClassifier([('rf', forest), ('lr',log)])\n",
    "\n",
    "ensemble_cv = cross_validate(ensemble, \n",
    "                            data_top20,\n",
    "                            y,\n",
    "                            scoring = \"f1_macro\",\n",
    "                            cv=3,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "ensemble_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) - Pipeline most steps (prepross & fit) in one single Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 346,
   "position": {
    "height": "40px",
    "left": "907px",
    "right": "20px",
    "top": "62px",
    "width": "401px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
